\documentclass[BSc, 12pt]{thesis/usydthesis}
\usepackage[margin=2.5cm]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{pdflscape}
\usepackage{xcolor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Daniel Bruwel}
\title{Navigating the Pachner Graph: Algorithms for Searching and Sampling Triangulations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% causes equations to be numbered as section.equation
\numberwithin{equation}{chapter}

% uncomment the following if you want the numbers of
% the theorems etc to appear on the left
% \swapnumbers

% I've set this up so that equations and theorems (etc)
% have a common numbering. If you don't want this change
% `equation' below to something else
\newtheorem{Definition}[equation]{Definition}
\newtheorem{Theorem}[equation]{Theorem}
\newtheorem{Proposition}[equation]{Proposition}
\newtheorem{Lemma}[equation]{Lemma}
\newtheorem{Corollary}[equation]{Corollary}

% This removes the italics
\theoremstyle{remark}
\newtheorem{Remark}[equation]{Remark}
\newtheorem{Example}[equation]{Example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% personal macros

% natural numbers, real numbers
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}

% macros for End(X) and Hom(X,Y)
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Hom}{Hom}

% for f:X -> Y; the default spacing isn't great
\newcommand{\map}[2]{\,{:}\,#1\!\longrightarrow\!#2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}    % start of the "text" in the document
\onehalfspacing


% roman page numbers for the initial pages
% \pagenumbering{roman}

% uncomment this to change the date on the title page
%\renewcommand{\Today}{October ????}
\maketitle          % creates the title page
\tableofcontents    % creates the table of contents


% uncomment the following if you want to put each chapter
%  into aseperate file

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \includeonly{chapt2}
% \include{intro}
%
% % reset the page numbering and change to arabic numbers
% \newpage\setcounter{page}{1}\pagenumbering{arabic}
%
% \include{chapt1}
% \include{chapt2}
% \include{chapt3}
% \include{chapt4}
%
% \include{references}
%
% \end{document}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Introduction}
% Big picture
The process of decomposing a surface or higher dimensional surface into discrete units such as triangles and tetrahedra is a standard technique used in a range of disciplines to take the analytic problem of studying a space or surface, to a combinatorial problem that opens it up to a range of computational techniques. Some examples of where triangulations are used includes physics such as the Regge calculus of general relativity \cite{Regge1961} and the causal dynamic triangulations of quantum gravity \cite{Ambjorn2004} where spacetime is triangulated for simulations; engineering where fluid and solid dynamics use ``meshes'' (triangulations) to approximate wings or bridges so that each component can be studied easily \cite{Courant1994}; computer graphics where objects are rendered with triangulations \cite{Gouraud1998}; and data science where the data can be used to generate a triangulation that can then be explored to reveal more about the system \cite{Zomorodian2004}. Triangulations also have many important purposes within mathematics itself, where they can be used to compute topological properties like homology groups and the Euler characteristic \cite{Hatcher2005}; differential geometry with fields such as combinatorial Ricci flows \cite{Chow2003}; and graph theory where concepts from triangulations were used in proofs such as the proof of the four colouring theorem \cite{Kenneth1977}. The main challenge with triangulations of surfaces and spaces, is that they are not unique, and different triangulations of a surface are better for different tasks. For example, finding the triangulation with the least faces helps in algebraic topology \cite{Hatcher2005}, whereas in computer graphics the triangulation that best approximates the geometry of the original surface is preferred. As such, considerable effort has been put into trying to explore and understand the world of triangulations of a given surface and find techniques to generate ``good'' triangulations.

% Narrowing focus
While there are a range of different notions of what is meant by good, this report primarily explores topological and combinatorial properties of triangulations. Specifically, we look for single vertex triangulations of the $3-$sphere that can be related to interesting or complex knots. The techniques developed can undoubtedly be extended to a range of different notions of goodness, but topological and combinatorial properties are fundamental to the triangulation itself, and do not depend on the specific geometry like the angles within or the size of specific triangles in the triangulation. This has the advantage of narrowing the focus of exploration to abstract triangulations which are discrete and countable, improving computational manageability and ensuring that the results are general and only dependent on the triangulation itself, not other erroneous geometric factors.

% Statement of aims
The primary objective of this report is to implement, analyse, and compare a range of different computational techniques for searching the space of triangulations. We investigate classical search and sampling techniques, namely Markov Chain Monte Carlo, greedy search, and simulated annealing. While greedy search algorithms have been a central tool to the study of this problem, Markov Chain Monte Carlo is a relatively new technique \cite{Altmann2025} and we introduce adaptations of this algorithm to simulated annealing. Additionally, we introduce the groundwork for a new approach that uses transformer-based models to explore the space, opening the door for reinforcement learning which is steadily developing a repertoire of success in a range of different problems, including learning to play games like Go \cite{Silver2016}, finding improved sorting algorithms \cite{Mankowitz2023}, finding improved solutions to $4\times4$ matrix multiplication and kissing number problems \cite{Novikov2025}, and helping with the development of new conjectures \cite{Davies2021}.

% Roadmap
The report is structured as follows: Chapter 1 brings together the necessary preliminaries, including manifolds, some knot theory, and an overview of various statistical and machine learning techniques. Chapter 2 provides a detailed outline and discussion of the specific algorithms that we are implementing, including justifications for the choices we made. Chapter 3 presents a description and analysis of a series of numeric experiments run across 5 different search problems and compares them. Finally, chapter 4 concludes with a discussion of our findings and outlines the direction for future research.

% reset the page numbering and change to arabic numbers
% \newpage\setcounter{page}{1}\pagenumbering{arabic}

\chapter{Preliminaries}
\section{Mathematical Preliminaries}
\subsection{Manifolds}
A $d-$\emph{dimensional manifold} or a $d-$\textit{manifold} is a topological generalisation of a surface. They are topological spaces that are ``well behaved'' and locally homeomorphic to $\mathbb{R}^d$. We recall the following basic definitions before precisely defining a manifold.

\begin{Definition}[Second Countable]
    A Topological space $(\mathcal{M}, \tau_\mathcal{M})$ is \textbf{Second Countable} if the topology $\tau_\mathcal{M}$ of $\mathcal{M}$ has a countable base.
\end{Definition}
\begin{Definition}[Hausdorff]
    A Topological space $(\mathcal{M}, \tau_\mathcal{M})$ is \textbf{Hausdorff} if for any two points $x\neq y$ in $\mathcal{M}$ there are neighbourhoods $U$ of $x$ and $V$ of $y$ such that $U\cap V=\varnothing$
\end{Definition}

With these we can define
\begin{Definition}[$d-$dimensional manifold]
    A $d-$\textbf{dimensional manifold} is a Hausdorff, second countable, topological space $\mathcal{M}$ where for every point $p\in\mathcal{M}$ there is a neighbourhood $U(p)$ of $p$ that is Homeomorphic to an open subset of Euclidean space $\mathbb{R}^d$.
\end{Definition}

\begin{Remark}
    We have used a homeomorphism to an open set of $R^d$. This is equivalent to saying that each point is either isolated (if $d=0$) or has a neighbourhood that is homeomorphic to all of $\mathbb{R}^d$.
\end{Remark}

\begin{Remark}
We will refer to an $d-$manifold simply as manifolds. Compared to other areas of research where we may call it a \textit{topological manifold} to emphasise that it does not have any further structure (c.f. \textit{differentiable manifold, Riemannian manifold, etc.}).
\end{Remark}

Some examples of manifolds include the circles $S^1$, the sphere $S^2$, the Torus $T^2$, and Euclidean space $\R^d$. Some non-examples include the disk $D^2$ because its boundary has no neighbourhood that is homeomorphic to an open subset of $\R^d$, the figure-eight curve because at the ``crossing'' there is no way to deform it to look like an open subset of $\R$, or the line with two origins because this is not Hausdorff.

In the study of triangulations we are often interested in a specific additional structure that a manifold can admit known as a \textit{piecewise-linear structure} or \textit{PL-structure}. Intuitively, a PL-structure allows a manifold to be realised as a collection of ``flat pieces'' joined together in in a linear fashion. For example $S^1$ admits a PL-structure that can be realised as the boundary of a triangle, or a square, etc. We say that two PL-manifolds (manifolds with PL-structure) are \textit{PL-homeomorphic} if they can be transformed into each other via a homeomorphism that is itself piecewise linear. This is all formally defined in terms of charts, atlases, and transition maps which can be found in most introductory books on geometric topology (e.g. ``Introduction to piecewise-linear topology '' \cite{Rourke2012}).

A particularly useful result due to Rad√≥ \cite{Rado1925} and Moise \cite{Moise1977} says that for dimension 3 or less PL-manifolds and PL-homeomorphisms are the ``same'' as manifolds and homeomorphisms. More precisely

\begin{Theorem}[Hauptvermutung]
    For dimension 3 or less, every manifold admits a unique PL-manifold up to PL-homeomorphism.
\end{Theorem}

\begin{Remark}
    This theorem does not hold for \textit{four} or more dimensions \cite{Milnor1961}.
\end{Remark}

We will be considering manifolds of dimension 3, so will often talk of a manifold and the unique class of PL-structures (up to PL-homeomorphism) that the manifold can admit as being the same thing.

\subsection{Triangulations}
We begin by defining \textit{simplices} - the fundamental building blocks of triangulations

\begin{Definition}[Simplex]
    An $d-$simplex $\Delta$ is the $d-$dimensional convex hull of $d+1$ vertices.
\end{Definition}

A $0-$simplex is a point, a $1-$simplex a line segment, a $2-$simplex a triangle, and a $3-$simplex a tetrahedron. One way to construct an $d-$simplex is by taking the \textit{join} of $d$ points where we recall that
\begin{Definition}[Join]
    For two topological spaces $X$ and $Y$, the join denoted $X*Y$ is constructed by taking $X\times Y\times[0,1]$ and taking the quotient by the equivalence relation $(x, y, 0)\sim(x,y',0)$ and $(x, y, 1)\sim(x',y,1)$ for $x,x'\in X$ and $y,y'\in Y$.
\end{Definition}
For for $i<d$, any subset of $(i+1)$ points of an $d-$simplex forms another simplex that we call an $i-$\textbf{dimensional face}.

\begin{Definition}[Boundary of a Simplex]
    For an $d-$simplex $\Delta$, the boundary denote $\partial\Delta$ is the union of all its $(d-1)-$dimensional faces.
\end{Definition}

We can takes simplices and ``glue'' them together to form more complex geometric structures. Formally this is done via \textit{face gluings}
\begin{Definition}[Face gluing]
    For two $d-$simplices $\Delta_1$ and $\Delta_2$ and two $i-$dimensional faces $F_1\subseteq \Delta_1$ and $F_2\subseteq \Delta_2$ we define the \textbf{face gluing} of $F_1$ to $F_2$ by taking $\Delta_1\sqcup \Delta_2/\sim$ where $\sim$ is defined by a \textit{gluing map} which is a homeomorphism between $F_1$ and $F_2$
\end{Definition}
We typically create the gluing map by taking a bijection between the vertices of $F_1$ and $F_2$ and linearly interpolating to create the full homeomorphism.

\begin{Definition}[PL-Sphere]
    The PL $0-$sphere is a pair of isolated points. For integers $n>0$ the PL $d-$sphere is a PL manifold PL-homeomorphic to the boundary of an $(d+1)-$simplex.
\end{Definition}

\begin{Definition}[PL-Triangulation]
    Construct the triangulation $T$ from a finite collection of $d-$ simplices $\{\Delta_1,...,\Delta_k\}$, called \textbf{facets}, by gluing their $(d-1)-$dimensional faces together. We call this a PL-triangulation if the following conditions are met
    \begin{enumerate}
        \item If a face is identified with itself as a result of the face gluings, the identification must happen along the identity map.
        % \item For all vertices $v$, the \textbf{link} defined as the set of all simplices $\Delta$ such that the join $v*\Delta$ is in $T$ forms a PL $n-1-$sphere
        \item For all vertices $v$, the \textbf{link} defined as the boundary of a small neighbourhood of $v$ forms a PL $(d-1)-$sphere
    \end{enumerate}
    This is a PL-triangulation of a manifold $\mathcal{M}$ if there is a PL-homeomorphism from $T\to\mathcal{M}$.
\end{Definition}

\begin{Remark}
    We can view PL-manifolds (manifolds with PL-structure) and PL-triangulations as the same thing in some sense. That being that for every PL-manifold there is a PL-triangulation that is compatible with it, and every PL-triangulation is a PL-manifold.
\end{Remark}

\section{Knot Theory}
For our preliminaries on knot theory, we will assume some knowledge on algebraic topology. There are many resources on the topic that can be used as reference, for example ``Algebraic Topology'' by Hatcher \cite{Hatcher2005}.

\begin{Definition}[Ambient Isotopy]
    For a pair of manifolds $N, M$ and embeddings $g, h$ of $N$ into $M$, an ambient isotopy from $g$ to $h$ is a continuous map $F:M\times[0,1]\to M$ such that $F_t:M\to M$ is a homeomorphism, $F_0$ is the identity, and $F_1\circ g=h$
\end{Definition}

\begin{Definition}[Knot]
    A knot is a smooth embedding of $S^1$ into $S^3$
\end{Definition}

\begin{Remark}
    We somewhat loosely refer to a knot as both the actual embedding function, and its image in $S^3$. However, if we say ``a knot $K$'', we typically mean the image of the embedding.
\end{Remark}

\begin{Definition}[Knot Equivalence]
    Two knots are said to be equivalent if there is an ambient isotopy between their embedding functions.
\end{Definition}

\begin{Definition}[Knot Invariant]
    A knot invariant is a property that can be assigned to a knot $K$ that is the same for all equivalent knots.
\end{Definition}

\begin{Remark}
    A knot invariant can be the same for multiple non-equivalent knots.
\end{Remark}

\begin{Definition}[Knot Complement]
    For a knot $K$, the tubular neighbourhood $\nu(K)$ is a small, closed, 3-dimensional region surrounding $K$ that is homeomorphic to the solid torus $S^1\times D^2$. The knot complement is $X_K=S^3\backslash \nu(K)$
\end{Definition}

We have that the boundary $\partial X_K\cong T^2$ is a torus. We take some time to define the Alexander polynomial, this is an important knot invariant central to this project. The following definitions are used only as preliminaries for definition~\ref{def:alex-poly} of the Alexander polynomial.

\begin{Definition}[Infinite Cyclic Cover]
    For the knot complement $X_K$, $\tilde{X}_K$ a cover of $X_K$ is an infinite cyclic cover if $\operatorname{Deck}(\tilde{X}_K/X_K)\cong\mathbb{Z}$
\end{Definition}
Every knot has a unique infinite cyclic cover. This follows from the fact that $H_1(X_K;\mathbb{Z})\cong Z$ and the properties of covering spaces.

\begin{Definition}[Alexander Module]
    For a knot $K$, take the infinite cyclic cover $\tilde{X}_K$ of the knot complement. The deck transformation group of this space is isomorphic to $\mathbb{Z}$, generated by some transformation $t$. Form the ring of Laurent polynomials $\Lambda=\mathbb{Z}[t, t^{-1}]$. The Alexander module is the first homology group $H_1(\tilde{X}_K)$ viewed as a module over $\Lambda$.
\end{Definition}

\begin{Definition}[Elementary Ideal]
    For a module $M$ over a unique factorisation domain $R$ with presentation of $n$ generators $g_i$ and $m$ relations $r_j=a_{j1}g_1+a_{j2}g_2+\dots=0$ we construct an $m\times n$ matrix $A$ of all the relations. The $k$th elementary ideal is the ideal $E_k(M)$ generated by all the $(n-k)\times (n-k)$ minors of $A$.
\end{Definition}
\begin{Theorem}
    The elementary ideals are independent of how the module is presented. So we refer to the elementary ideals of the module without reference to the presentation.
\end{Theorem}

\begin{Definition}[Alexander Polynomial]\label{def:alex-poly}
    The Alexander Polynomial is the generator of the first elementary ideal of the Alexander module.
\end{Definition}

\begin{Theorem}
    The Alexander polynomial is a knot invariant.
\end{Theorem}
The proof of the above is due to J. W. Alexander \cite{Alexander1928}.


\subsection{Isomorphism Signatures}
We say that two PL-triangulations are \textit{combinatorially isomorphic} if they are the same up to relabelling of their faces. Because there is no interesting mathematical information in the labelling, we typically define a canonical labelling. While the exact details of constructing a canonical labelling are specific and can depend on the software choice, the general idea is as follows.
\begin{enumerate}
    \item For each facet, calculate some label independent invariant (e.g. the number of unique facets being glued to).
    \item Partition the facets into labelled bins based on this invariant (this labelling is canonical based on the value of the invariant)
    \item For each facet in each bin (containing multiple facets), look at which bins the facets neighbouring facets (from gluing) are in. Use this information to construct a new label. If there are facets in the same bin that get a different new label, split the bin in some canonical way and create a new labelling.
    \item Repeat the above process, constructing a splitting tree. This tree is canonical. This tree is not guaranteed to split the bins into singletons. If it gets ``stuck'', make an arbitrary choice and continue the algorithm until everything falls into a singleton. This is a possible canonical labelling.
    \item If an arbitrary choice had to be made (or perhaps multiple), rerun the entire algorithm for each possible choice that could have been made, creating a small list of possible canonical labellings. Chose the lexicographically smallest labelling.
\end{enumerate}
The choice of canonical labelling used for the rest of this paper is as in the $3-$manifold software Regina \cite{Regina}. The details of the initial invariant, how the bins are labelled, and optimisations can be found in the corresponding paper \cite{Regina}. In the worst case scenario this process takes $\mathcal{O}((d+1)!\cdot n^2)$ where $d$ is the dimension and $n$ is the number of facets. For a $3-$dimensional triangulation the $(d+1)!$ term is small and this algorithm is fast.

Once a canonical labelling is found, an ``isomorphism signature'' is calculated. For a $3-$ dimensional triangulation, the only information that needs to be stored is the number of tetrahedra, and for each face of each tetrahedron what the destination tetrahedron is, and what the vertex order is. We do not need to specify the destination face because this can be determined by a combination of permutation labelling, looking at where the destinations faces are being mapped, and strict ordering. To efficiently store this, the following is done.
\begin{enumerate}
    \item For each face of each tetrahedron, calculate $v_i=6\cdot\text{destination id} + \text{permutation id}$. This is valid due to there being 6 vertex permutations. This value will be less than $6k$ for $n$ facets.
    \item Create a sequence $(v_0, v_1, \cdots, v_{4n-1})$ ordered based on the canonical labelling.
    \item Compute $I=v_0+v_1(6n)+v_2(6n)^2+\cdots$
    \item Convert $I$ to base 64 and store as a string.
    \item Convert $n$ to some string and append to the front of the string representation of $I$.
\end{enumerate}
This process forms a string / number representing the unique combinatorial isomorphism signature for any given triangulation of a $3-$manifold (practically there are size restrictions on the number of facets $n$).

\subsection{Pachner Moves and the Pachner Graph}
A Pachner move is a specific process that changes a PL-triangulation without changing the underlying topological space.

\begin{Definition}[Complementary Triangulation]
    Let $\Delta$ be a $(d+1)$simplex, let $A$ be a a connected subcomplex of $\partial \Delta$ - that is a set of $d-$simplexes that form a connected topological space. The \textit{complementary triangulation} of $A$ is $B=\partial\Delta \backslash A$.
\end{Definition}

From this we can abstractly define a Pachner move

\begin{Definition}[Pachner Move]
    For a $d$-dimensional PL-Manifold, identify an $i-$dimensional simplex $\sigma^i$. The set of all facets that contain $\sigma^i$ forms a subcomplex of the boundary of some $(d+1)-$dimensional triangulation. Replace this set with the complementary triangulation of this sub complex. This Pachner move is called an $i-$move.
\end{Definition}

We have the following proof due to Pachner \cite{Pachner1991}
\begin{Theorem}\label{thm:pachner-connected}
    Two triangulations $T$ and $T'$ are reachable in finitely many Pachner moves if and only if they represent the same PL-manifold.
\end{Theorem}

For triangulations of $3-$manifolds, we can take $i=0, 1, 2, 3$, where $i=0, 3$ are inverses of each other, and likewise $i=1, 2$. For a $2-$move, we identify a triangle and find all the tetrahedra that share that triangle, there are two of these, we replace these with 3 tetrahedra that now share a common edge. The $1-$move is the inverse of this. We often call these moves $(3,2)$ and $(2,3)$ moves to explicitly state how we are going, for example, from $3$ tetrahedra to $2$. For a $0-$move, we identify a vertex with $4$ tetrahedra that share it, and replace these with a single tetrahedron. The $3-$move is the inverse. These are often called $(4,1)$ and $(1,4)$ moves as above.

Because all triangulations of a specific PL-manifold can be reached through Pachner moves, we define

\begin{Definition}[Pachner Graph]
    For a given PL-manifold $\mathcal{M}$, the Pachner graph for $\mathcal{M}$ us the graph $(V, E)$ given by $V=\{\text{triangulations of }\mathcal{M}\}$ and for $v_i, v_j\in V$, $e_{ij}=(v_i, v_j)\in E\iff v_i, v_j$ are related via a single Pachner move.
\end{Definition}

Because of theorem~\ref{thm:pachner-connected}, the Pachner graph is connected for any PL-manifold. The Pachner graph is, however, infinite. We additionally endow the Pachner graph with ``levels'' where each level encodes the number of facets in the triangulation.

\begin{Theorem}
    The size of the Pachner graph grows super exponentially in its level.
\end{Theorem}

We notice that $(4,1)$ and $(1,4)$ moves change the number of vertices, but the $(3,2)$ and $(2,3)$ moves do not. We are often interested in ``single vertex'' triangulations. For these we can exclusively use $(3, 2)$ and $(2,3)$ moves. A particularly useful result due to Matveev \cite{Matveev2007} is that

\begin{Theorem}
    Any single vertex triangulation of the $3-$sphere with at least 2 tetrahedra $S^3$ can be reached from any other such triangulation through only $(3,2)$ and $(2,3)$ moves.
\end{Theorem}

We analogously define the Pachner graph for $1-$vertex triangulations of $S^3$, which is once again a connected graph. It is currently unknown if the Pachner graph for $1-$vertex triangulations of $S^3$ grows super exponentially or exponentially in its level.

\section{Machine Learning Preliminaries}
\subsection{Computational Complexity}
\begin{Definition}[Time Complexity]
    For an algorithm with input size $n$, the time complexity is the asymptotic worst case number of steps the algorithm needs to process its input, written in ``Big-O'' notation as $\mathcal{O}(g(n))$.
\end{Definition}

\begin{Definition}[Space Complexity]
    For an algorithm with input size $n$, the space complexity is the asymptotic worst case amount of memory the algorithm needs to allocate when processing its input, written in ``Big-O'' notation as $\mathcal{O}(g(n))$.
\end{Definition}

\begin{Definition}[Decision Problem]
    A decision problem is a type of problem that can be answered as either ``yes'' or ``no''
\end{Definition}


\begin{Definition}[Decidable]
    A decision problem is \textbf{decidable} if there exists an algorithm that can always provide the correct answer for any given input in a finite amount of time. Conversely the problem is \textbf{undecidable} if it is not decidable.
\end{Definition}

\begin{Remark}
    While most problems are not decision problems, it is often possible to convert many problems into a decision problem. For example, if the problem is to multiply two numbers together, we can convert this to a decision problem of ``is the $i$th binary digit a $0$?'', this can be repeated for all digits.
\end{Remark}

\begin{Definition}[Complexity Class]
    A complexity class is a class of problems that can be solved under some model of computation within some resource bound i.e. some time or space bound. A problem belongs to a given complexity class if there exist an algorithm to solve the problem within this bound. For ``turning machines'', the main complexity classes are
    \begin{enumerate}
        \item $L$: The problem can be solved using a logarithmic amount of space.
        \item $NL$: A solution can be verified using a logarithmic amount of space.
        \item $P$: The problem can be solved in polynomial time.
        \item $NP$: A `yes' solution can be verified in polynomial time.
        \item $\operatorname{co}NP$: A `no' solution can be verified in polynomial time.
        \item $PSPACE$: A solution can be found using a polynomial amount of space.
        \item $EXPTIME$: A solution can be found using an exponential amount of time.
    \end{enumerate}
    There exist many other complexity classes for both turning machines and other models for computation.
\end{Definition}

\begin{Remark}
    $NP$ and $\operatorname{co}NP$ are distinct. For example for a graph $G$, the problem of if there is a path that visits all nodes exactly once is $NP$ because I can confirm that a path exists if you simply give me one, but it is believed to not be $\operatorname{co}NP$ as this would mean I could easily verify based on some example path that there can't be a path that visits all nodes exactly once.
\end{Remark}

\begin{Remark}
    A turning machine is a common model for computation, we do not define it here but there are many resources available.
\end{Remark}

\begin{Remark}
    We have defined $NL$ and $NP$ as having solutions verifiable in a specific bound, this means that if the question is for example, is there a path of length $k$ between two nodes, someone can give a path that is a solution, and it is possible to verify that this connects the two nodes and has length $k$. Formally $NL$ and $NP$ are not defined this way, but instead using things called ``non-deterministic Turing machines'', but the definitions can be shown to be equivalent.
\end{Remark}

\begin{Theorem}
    $L\subseteq NL\subseteq P\subseteq NP\subseteq PSCPAE\subseteq EXPTIME$
\end{Theorem}

\begin{Remark}
    There are conjectures that some of these subsets may be proper subsets or equalities, most famously the question of if $P=NP$, but there are not as of yet any solutions.
\end{Remark}

\begin{Definition}[Hard Problem]
    For a given complexity class $C$, a problem $P$ is called a $C-$hard problem if every problem $P'\in C$ can be ``reduced'' to $P$ via some algorithm that is ``efficient'' relative to $C$.
\end{Definition}

\begin{Remark}
    We do not formally define what it means to be ``efficient'' relative to $C$ as it is getting into the details too much, but for $NP$ and $PSCPAE$ problems, a polynomial time reduction is efficient.
\end{Remark}

\begin{Definition}[Complete Problem]
    For a given complexity class $C$, a problem $P$ is called a $C-$complete problem if it is a $C-$hard problem and $P\in C$
\end{Definition}

Practically, problems that belong to $P$ can be solved tractable on a modern computer, we can typically scale resources and solve the problem. However, all the algorithms we have for $NP$ complete problems require exponential resources to run, which becomes computationally infeasible quickly. As such, if we find a problem to be $NP-$complete or harder, it means we can't just ``throw more compute'' at the problem to solve it and may have to resort to suboptimal solutions using heuristics etc.

From these definitions we can discuss a few important problems that we will encounter, and their respective complexity. The general high level of complexity of many of these problems is a motivating factor for our algorithms.

\begin{Theorem}
    Finding the shortest path between two vertices $v_i$ and $v_j$ of the Pachner graph is PSPACE complete.
\end{Theorem}

\begin{Theorem}\label{thm:trivial-presentation-undecidable}
    Deciding if a presentation of a group is trivial is undecidable.
\end{Theorem}

\begin{Theorem}
    Deciding if $3-$dimensional triangulation is $S^3$ is in $NP\cap \operatorname{co}NP$
\end{Theorem}

\begin{Theorem}
    Checking if a knot is the unknot is $NP\cap \operatorname{co}NP$
\end{Theorem}

\subsection{Markov Chain Monte Carlo}
\begin{Definition}[Markov Chain]
    A \textit{Markov Chain} is a sequence of values $x(t)$ indexed by ``time'' where the sequence has the \textit{Markov Property} - that is $x(t+1)$ depends only on $x(t)$, typically probabilistically.
\end{Definition}

Monte Carlo methods are a class of methods that use random sampling to approximate something. Often, they are used to approximate the integral of some complex function, however not exclusively.

\textit{Markov Chain Monte Carlo} is a class of methods to sample from a distribution by constructing a Markov Chain that's steady state solution is the distribution. More formally we define

\begin{Definition}[Markov Transition Kernel]
    Take $(\mathcal{X}, \mathcal{F})$ a measurable space with $\mathcal{F}$ a sigma algebra. A \textit{Markov Transition Kernel} is a function $P:\mathcal{X}\times\mathcal{F}\to[0, 1]$ such that
    \begin{enumerate}
        \item for any $x\in\mathcal{X}$, the function $A\mapsto P(x, A)$ is a probability measure on $(\mathcal{X}, \mathcal{F})$
        \item for any $A\in\mathcal{F}$, the function $x\mapsto P(x, A)$ is a measurable function.
    \end{enumerate}
\end{Definition}

The Markov Transition Kernel is used to ``evolve'' the state. That is, if you are currently ``located'' at $x \in \mathcal{X}$, $P(x, A)$ tells you the probability of ``ending up'' in $A$. More precisely, if at time $t$ the system is distributed according to $\nu_t$, then the distribution at $t+1$ is
\begin{equation}
    \nu_{t+1} = \nu_t P(A)=\int_\mathcal{X}P(x, A)d\nu_t(x)
\end{equation}

\begin{Definition}[Target Distribution]
    $\pi$ is called a target distribution for a Markov Transition Kernel $P$ if the following two conditions hold
    \begin{enumerate}
        \item \textbf{Stationarity}: $\pi P=\pi$
        \item \textbf{Ergodicity}: For $\pi-$almost all points $x_0$, $$\lim_{n\to\infty}\|P^n(x_0,\cdot)-\pi(\cdot)\|_{TV}=0$$
    \end{enumerate}
\end{Definition}

\begin{Remark}
    A statement is true for ``$\pi-$almost all $x_0$'' means that it is true for all $x_0$ except a set that has measure $0$ under $\pi$
\end{Remark}

Typically proving stationarity and ergodicity is difficult, so the following is used instead

\begin{Theorem}
    For a Markov transition kernel $P$, if the following are true for some distribution $\pi$
    \begin{enumerate}
        \item \textbf{$\pi-$irreducibility}: For any $A\in\mathcal{F}$ with $\pi(A)> 0$, there exists some $n$ such that $P^n(x, A)>0$ for all $x\in\mathcal{X}$
        \item \textbf{Aperiodicity}: The chain is aperiodic
        \item \textbf{Recurrent}: For any measurable set $A\in\mathcal{F}$, and any starting point $x\in\mathcal{X}$ the hitting time $\tau_A$ is finite almost surely, where $\tau$ is the amount of steps to go from $x$ to $A$.
        \item \textbf{Reversibility}: For any two $A, B\in\mathcal{F}$ we have $$\int_AP(x, B)d\pi(x)=\int_BP(x,A)d\pi(x)$$
    \end{enumerate}
    then $\pi$ is the target distribution for the Markov transition kernel $P$.
\end{Theorem}

In particular, if we have some process $\hat{P}$ that takes in some $x(t)$ and produces some $x(t+1)=\hat{P}(x(t))$ according to some Markov transition kernel $P$, we can create a Markov Chain, by continuously generating samples, and applying $\hat{P}$ to them recurrently, these samples will be ``distributed according to'' $\pi$. More precisely, the partial sums $S_n=\frac{1}{n}\sum_i^n f(x_i)\to \mathbb{E}^\pi[f]$ at a ``rate'' $\mathcal{O}(\sqrt{N})$.

\begin{Definition}[Metropolis Hastings]
    For some function $f$, not necessarily a probability density, and some ``proposal function'' $g$ the Metropolis Hastings algorithm on $f$ with proposal $g$ is the following:
    Initialise some $x_0$, for each $t$ do the following
    \begin{enumerate}
        \item Propose some $x'$ according to $g(x'|x_t)$
        \item Compute $\alpha = f(x')/f(x_t)$
        \item Sample some $u\in[0, 1]$ uniformly
        \item If $\alpha > u$ accept $x'$ by setting $x_{t+1}=x'$, otherwise set $x_{t+1}=x_t$
    \end{enumerate}
\end{Definition}

\begin{Theorem}
    If the proposal function $g$ is symmetric - that is $g(x|y)=g(y|x)$, and if $f$ is proportional to some probability distribution $\pi$, then the Metropolis Hastings algorithm for $f$ with proposal $g$ will generate samples according to $\pi$.
\end{Theorem}

\begin{Remark}
    If the proposal distribution is not symmetric, a factor known as the Hastings ratio can be introduced to find $\alpha_H=\alpha\cdot\frac{g(x'|x)}{g(x|x')}$ which is used in the acceptance step.
\end{Remark}

\subsubsection{Statistics in MCMC}
While the Metropolis Hastings algorithm (and other MCMC algorithms) converge to $\pi$ as $n\to\infty$, for finite $n$ the distribution will not necessarily be $\pi$, a common example of this is if $\pi$ is bimodal, where sampling can get ``stuck'' in one of the modes of the distribution, thereby not fully exploring the space. As such, there are a number of statistics that are used to check if a finite sample from an MCMC algorithm is likely to have converged to the proper distribution $\pi$. This is typically done by running multiple independent chains of MCMC.

\begin{Definition}[Variance]
    Let there be $J$ chains of $L$ samples, with samples $x_1^j, ..., x_L^j$ for the $j$th chain. We have the
    \begin{enumerate}
        \item Between chain variance: $B=\frac{L}{J-1}\sum(\bar{x}_j-\bar{x}_*)^2$
        \item Within chain variance: $W=\frac{1}{J}\sum\left(\frac{1}{L-1}\sum(x_i^j-\bar{x}_j)^2\right)$
    \end{enumerate}
\end{Definition}

The between chain variance follows from the central limit theorem that $\operatorname{Var}(\bar{x}_j)\approx \sigma^2\cdot n$, so $B=\sigma^2=L\cdot\operatorname{Var}(\bar{x}_j)$, and $\frac{1}{J-1}\sum(\bar{x}_j-\bar{x}_*)^2$ is an unbiased estimate of the variance. The within chain variance is simply the average unbiased estimate of the variance of each chain. From these we can derive

\begin{Theorem}[Gelman-Rubin Variance Estimate]
    Let there be $J$ chains of $L$ samples, with samples $x_1^j, ..., x_L^j$ for the $j$th chain. We have that the total variance is
    $$\hat{V}=\frac{L}{L-1}W+\frac{1}{L}B$$
\end{Theorem}
This result follows directly from the law of total variance.

From this we can construct
\begin{Definition}[Gelman-Rubin Statistic]\label{def:gelman-rubin}
    Let $\hat{V}$ be the Gelman-Rubin variance estimate, and $W$ be the within chain variance. The Gelman-Rubin statistic is
    $$\hat{R}=\sqrt{\frac{\hat{V}}{W}}$$
\end{Definition}
When the chains have converged, each chain is a valid sample from $\pi$, as such $W=\hat{V}$ and $\hat{R}=1$. However, if one or more of the chains has not suitably explored the space, $W<\hat{V}$ and $\hat{R}>1$. The samples will never be a complete representation of the distribution, so typically a threshold of $\hat{R}=1.01$ is used to indicate convergence.


\subsection{Simulated Annealing}
For a measurable space $(\mathcal{X}, \mathcal{F}, \mu)$ with base measure $\mu$ (typically the Lebesgue, or count measure), we may have some measurable function $E:\mathcal{X}\to\mathbb{R}$ known as the energy. In thermal physics, for a thermodynamic system at temperature $T$, the probability of finding a particle in a given energy $E$ is given by the Gibbs distribution $\pi_{T}(A)=\frac{1}{Z}\int_Ae^{-E(A)/T}d\mu(x)$ where $Z=\int_\mathcal{X} e^{-E(A)/T}d\mu(x)$ is a normalising factor. Typically as a system ``cools down'' it ``finds its way'' to low energies. Simulated annealing is inspired by this thermodynamic concept, simulating a particle over time as the temperature $T\to 0$ heuristically conjecturing that it will find its way to the lowest energy state. The simulation at a given temperature $T$ is typically done using the Metropolis Hastings algorithm.

While the concept above is inspired as a Heuristic from concepts in thermal physics, it has valid grounding in probability and machine learning. At a high level, this is because for any $x, x'\in \mathcal{X}$, $\pi_T(x')/\pi_T(x)=e^{-(E(x')-E(x))/T}$, which if $E(x') > E(x)$ tends to $0$ as $T\to 0$, but if $E(x') < E(x)$ tends to $\infty$. As such, a sharp delta function is formed around $x_{min}=\arg\min(E)$. While this does mean that at $T=0$ we have that $\pi(x)=\delta(x-x_{min})$ is the stationary solution to our Metropolis Hastings Markov Transition Kernel, at $T=0$ the measure of valid starting points $\{x_0\}$ tends to have measure $1$ under $\pi$ but measure $0$ under $\nu$. Informally, at low $T$ we almost always accept points where $E(x')<E(x)$ and almost never accept points where $E(x')>E(x)$, so this is just hill climbing and unless $E$ is convex, the sampling will likely get stuck in a local minima.


\subsection{Transformers}\label{subsec:transformers}
\begin{Definition}[Row Operator]
    For a field $F$, and a function $s:F^d\to F^d$ define the row operator $\mathcal{R}_s:M_d(F)\to M_d(F)$ as the act of applying $s$ to each row of $M_d(F)$. That is $$\mathcal{R}_s\left((r_1, r_2, ..., r_d)^T\right)=\left(s(r_1), s(r_2),...,s(r_d)\right)^T$$
\end{Definition}

\begin{Definition}[Abstract Transformer Head]
For a collection of vectors $(v_1, v_2, ..., v_d)$ where $v_i\in V$ an vector space over a field $F$. Form $\dot{V}$ by equipping $V$ with a bilinear form denoted $B:V\times V\to F$. Let $G:F^d\otimes V\to M_d(F)$ denote the ``Gram operator''. Let $L:V\to W$ be an arbitrary linear operator, and $s:F^d\to F^d$ an arbitrary function.

An abstract transformer head is a function $H:F^d\otimes V\to F^d\otimes W$ defined by
\begin{equation}
    H(x)=(\mathcal{R}_f(G(x))\otimes L)(x)
\end{equation}
\end{Definition}

\begin{Definition}[Abstract Multi-Head Attention]
For a collection of vectors $(v_1, v_2, ..., v_d)$ where $v_i\in V$ an vector space over a field $F$. Form spaces $V_1, V_2, ..., V_q$ from $V$ by equipping $V$ with a, possibly unique, inner product. Let $\iota_i:V\to V_i$ be the operation of endowing $V$ with the inner product structure of $V_i$. Let $H_i$ denote the transformer head from $F^d\otimes V_i\to F^d\otimes W_i$ where $W_i, W_j$ may be distinct. Define $O:W_0\oplus W_1\oplus \cdots\oplus W_q\to V$

The multi-head attention is an operation $A:F^d\otimes V\to F^d\otimes V$ defined as
\begin{equation}
    A(x)=(I\otimes O)\left(\bigoplus_i H_i(\iota_i(x))\right)
\end{equation}
\end{Definition}

\begin{Definition}[Abstract Feedforward Neural Network]
For a vector space $U$ over a field $F$, a Feedforward is a operator $N:U\to W$ for a pair of linear operators $L_1:U\to V$ and $L_2: V\to W$ and a non-linear function $a:V\to V$ is $N(x)=L_2(a(L_1(x)))$
\end{Definition}

\begin{Definition}[Abstract Transformer Block]
    For a collection of vectors $(v_1, v_2, ..., v_n)$ where $v_i\in V$ an vector space over a field $F$. A transformer block $T:F^d\otimes V\to F^d\otimes V$ is an operator defined as
    $$T(x)=x + A(x)+(I\otimes N)(x+A(x))$$
\end{Definition}

While these abstract formulations of the Transformer are theocratically rich, in practice we are working over $F=\mathbb{R}$ and our vector space $V$ is finite dimensional. This allows us to write $V=\mathbb{R}^{d_e}$, write all the linear transformations as matrix multiplications, and write the bilinear forms as $B(v_1, v_2)=v_2^TMV_1$ for some matrix $M$. Also, the non-linear functions used such as $f$ in the abstract transformer head, and $a$ in the feedforward neural network are fixed, or depend on some small set of learnable parameters. This gives a finite set of scaler parameters $\theta$ that defines the entire transformer block. Additionally, some further components are introduced into the transformer block to ensure computationally stability and regularisation. This lets us define

\begin{Definition}[Standard Transformer Head]
    For a collection of vectors $(v_1, v_2, ..., v_d)$ where $v_i\in \mathbb{R}^{d_e}$ we construct $X\in\mathbb{R}^{d\times d_e}$ by stacking these vectors up as columns. The head is calculated as

    \begin{equation}
        h(X) = \operatorname{softmax} \left( \frac{(XW^Q)(XW^K)^T}{\sqrt{d_k}}+M \right)(XW^V)
    \end{equation}

    Where the $\operatorname{softmax}$ is applied along each row. The $W^Q, W^K, W^V\in\mathbb{R}^{d_e\times d_k}$ - where $d_k$ is some reduced dimension, typically chosen to be a factor of $d_e$ - are called the ``query'', ``key'', and ``value'' weights. $M\in M_d(\mathbb{R}\cup\{-\infty, \infty\})$ is called the causal mask.
\end{Definition}

\begin{Definition}[Standard Multi-Head Attention]
    For $X$ defined as above
    \begin{equation}
        MHA(X, h)=\operatorname{Concat}(h_1(X), ..., h_h(X))W^O
    \end{equation}
\end{Definition}

\begin{Definition}[Standard Feedforward Neural Network]
    For $x\in \mathbb{R^n}$, $FFN(x)=\sigma(xW_1+b_1)W_2+b_2$ where the $W_i$ are called weights and the $b_i$ are called biases. $\sigma$ is a parameter-less ``activation function''.
\end{Definition}

\begin{Definition}[Layer Norm]
    For a vector $x\in \mathbb{R}^d$ define $\mu$ and $\sigma$ as the mean standard deviation of $x$. The layer norm of $x$ is
    \begin{equation}
        LN(x)_i=\gamma_i\cdot\left(\frac{x_i-\mu}{
        \sqrt{\sigma^2+\epsilon}
        }\right) + \beta_i
    \end{equation}

    Where $\gamma, \beta\in\mathbb{R}^n$ are learned parameters, and $\epsilon$ is some fixed number for numerical stability.
\end{Definition}

\begin{Definition}[Dropout Layer]
    For $r\in[0, 1]$ the dropout of a vector or matrix $X$ is $DO(X, r)$ which is equal to $X$ in all but $\operatorname{round}(r|X|)$ positions which have randomly been selected and set to zero. Here $|X|$ is the number of entries in $X$.
\end{Definition}

\begin{Remark}
    Layer norm and dropout were not included in the abstract definition of the transformer block as it is primarily used for numerical stability.
\end{Remark}

\begin{Definition}[Standard Transformer Block]
For $X$ as defined above

\begin{align*}
    TB(X, h, r) = DO(X_{MHA}+FFN(LN(X_{MHA})), r)
\end{align*}
Where $X_{MHA} = DO(X+MHA(LN(X), h)), r)$
\end{Definition}

Because these transformer blocks map from $F^d\otimes V\to F^d\otimes V$, or from $\mathbb{R}^{d\times d_e}\to \mathbb{R}^{d\times d_e}$, they can be chained together.

For many tasks which transformers excel in (language modelling, time series forecasting) the input sequence is sequential, and often not encoded into a vector in a meaningful way. As such we have

\begin{Definition}[Token Embedding]
    For an input vectors $v\in\mathbb{R}^V$, the token embedding of $v$ is $h=x^TE$ where $E\in\mathbb{R}^{V\times d_e}$.
\end{Definition}

\begin{Remark}
    For tasks where the input is categorical, $v$ is usually formed with some one-hot-encoding, that is if there are $V$ categories we define for category $c$ $v_c=(0,...,0,1,0,...,0)$ as a vector in $\mathbb{R}^V$ that has a $1$ in the position corresponding to category $c$ (arbitrary), and zeros elsewhere.

    Often (e.g. language modelling) there are many categories, so $d_e<V$ and the embedding becomes a useful technique for encoding ``semantic'' meaning into a vector.
\end{Remark}

Once we have embedded our vectors, there is often a sequential order to them, i.e $(v_0, v_1, v_2, ...)$. The transformer block as described above, is invariant under changing of this order. As such, we use
\begin{Definition}[Positional Encoding]
    For a sequence of vectors indexed by time, $(v_0, v_1, ..., v_d)$ each $v_i\in\mathbb{R}^{d_e}$. For $D_{max}$ the longest possible sequence, and a positional embedding matrix $P\in\mathbb{R}^{D_{max}\times d_e}$, the positional embedding of $(v_0, v_1, ..., v_d)$ is defined with $v_i\mapsto v_i+P_i$ where $P_i$ is the $i$th row.
\end{Definition}

A prototypical example of applying these steps is in GPT-2, described as follows. For a sequence of words $(w_1, w_2, ...,w_d)$, each word in a vocabulary of size $V$, encode each word via one hot encoding. Perform token embedding into $\mathbb{R}^{d_e}$, and perform a positional encoding. Apply a series of transformer blocks sequentially, apply a single layer norm, apply a linear projection back into $\mathbb{R}^V$ for each ``word'' (via some matrix multiplication), and finally apply softmax to each output ``word''. The final vector is trained to represent the probability of the next word in the sequence of words. So for position $1$ the prediction is for $w_2$, for position $d$ the prediction is for word $w_{d+1}$ not in the data. To train GPT-2 we use an objective function that calculates the cross entropy between the prediction at each step and the actual next word, this is done for each ``word'' in the output layer.

\subsection{Gradient Descent}
If we have a neural network $N(x\;|\;\theta)$ that takes in some input $x\in V$ and some parameter set $\theta\in\mathbb{R}^d$ and produces some output $y\in W$ where $V, W$ are vector spaces over a field $F$, typically $\mathbb{R}$, and typically finite dimensional. We typically have some ``training data'', which is a set of $\{(x_i, y_i)\;|\;x_i\in V, y_i\in W\}$ where each $y_i$ is the output that we ``want'' from $N(x_i, \theta)$, what we mean by ``want'' is that some function $\mathbb{E}_{x_i}[\mathcal{L}(N(x_i, \hat{\theta}), y_i)]$ is minimised by our parameter choice $\hat{\theta}$. To find this target $\hat{\theta}$. Because $N$ is typically complicated, and has a complex dependence on $\theta$, we cannot directly minimise this objective function to find $\hat{\theta}$, as such we use gradient decent. The concept behind gradient decent is to take some $\theta_i$, find a local, linear approximation of $\mathbb{E}_{x_i}[\mathcal{L}(N(x_i, \hat{\theta_i}), y_i)]$, and find some ``penalty'' for ``trusting'' the approximation too much. Precisely, we compute
\begin{equation}
    \theta_{i+1}=\arg\min_\theta \left(\langle g^T, (\theta-\theta_i)\rangle+\frac{\lambda}{2}\|\theta-\theta_i\|^2\right)
\end{equation}

This is done recursively, typically for some number of steps. Here $g$ is the gradient, often we would consider using $\nabla_\theta \mathbb{E}_{x_i}[\mathcal{L}(N(x_i, \hat{\theta_i}), y_i)]$, but computing this over the entire ``training'' data set is slow, so we typically a small ``batch'' of data is taken from the training data and the gradient is taken over this batch. Because this batch is ``random'' a small subset of the true data, our estimate of the gradient is going to be somewhat stochastic, as such many gradient decent algorithms use some sort of exponential smoothing step for the gradient - this is often called the momentum. Different choices of exponential smoothing, and different choices of the norm, and different choices of the gradient lead to different types of optimisers. A few common ones are \textit{SGD} which uses the frobenius norm, the standard gradient, and no smoothing; \textit{adam}, which uses the $\ell_1\to\ell_\infty$ norm, the standard gradient, and exponential smoothing; \textit{shampoo}, which uses the spectral norm, the standard gradient, and exponential smoothing; and \textit{NGF} which uses the frobenius norm, the natural gradient, and no exponential smoothing.

\subsection{Reinforcement Learning}
For our specific usecase, reinforcement learning is a technique used when we have some ``reward function'' $R(y): Y\to\mathbb{R}$ that tells us how good a specific sample is, and we want to update the parameters of some probability distribution / sampling function $Y\sim\pi_\theta$ to maximise the expected reward, that is

\begin{equation}
    J(\theta)=\mathbb{E}_{y\sim\pi_\theta}[R(y)]
\end{equation}

While reinforcement learning is typically more general than this, and applies to policy learning, this will suffice for our purpose.

The most direct way to update $\pi_\theta$ is to simply take a number of samples from $\pi_\theta$ and calculate

\begin{equation}
    g=\nabla_\theta J(\theta)=\mathbb{E}_{y\sim\pi_\theta}[\nabla_\theta \ln(\pi_\theta) R(y)]
\end{equation}

and then use this $g$ in gradient decent as described above.

While the above techniques makes logical sense, the use of $J(\theta)$ being the expected reward can become problematic, for example if our reward function is binary, returning a score of $1$ if the output meets some criteria, and returning a score of $-1$ if it does not, once the parameter set has settled into a stable configuration where each output is valid and produces a score of $1$, $J(\theta)$ continues to update $\theta$ which can lead to oscillations. As such we often use an ``advantage'' function $A$ which is an estimate of how the score $R(y_i)$ for a specific sample compares to the expected score. That is $A(y_i)=R(y_i)-\mathbb{E}_{y\sim \pi_\theta}[R(y)]$. We can estimate the expected score by simply sampling from $\pi_\theta$ a few times, and taking the average, and for a simple problem this is sufficient.

\begin{Remark}
    This average often has a high variance, so a ``critic'' model is often trained instead to try and learn the baseline. We won't explore this technique here.
\end{Remark}

We can use the advantage to get a refined objective function $J(\theta)=\mathbb{E}_{y\sim\pi_\theta}[A(y)]$. We can calculate the gradient once again as above, and apply gradient accent. The primary disadvantage here is that each time we take a gradient step, we have to resample from $\pi_\theta$ to get a new batch to calculate the new gradient. The new distribution with parameter set $\theta$ will only be slightly different to the original parameter set before the gradient update $\theta_{old}$, and as such we can keep the old samples $y_i\sim\pi_{\theta_{old}}$ and then use a technique called importance weighting to find that
\begin{equation}
    \mathbb{E}_{y\sim\pi_\theta}[A(y)]=\mathbb{E}_{y\sim\pi_{\theta_{old}}}[r(\theta)A(y)]
\end{equation}

Where $r(\theta)=\frac{\pi_\theta(y)}{\pi_{\theta_{old}}(y)}$ is called the importance ratio. This allows us to take batch of samples from $\pi_{\theta_{old}}$, perform a few steps of gradient accent using this batch, importance reweighing at each step. While this works in expectations, typically after a few gradient steps our samples are no longer representative, and our reweighing erodes. Therefore, it is typical to use a ``trust region'' wherein we can trust an update. One of the most common ways to do this is to ``cap'' positive updates by restricting $r(\theta)\leq 1+\epsilon$ if $A(y)>0$, that is if if we generated good samples from $\theta_{old}$ we want to increase the likelihood of generating them, but not so much so that we can no longer trust the importance sampling as the ratio $r(\theta)$ has become too big, but on the other hand, if the advantage $A<0$, we want to continue to have a learning signal. This yields the proximal policy optimisation (PPO) objective
\begin{equation}
    L^{CLIP}(\theta)=\mathbb{E}_{y\sim\pi_{\theta_{old}}}[\min\left(r(\theta)A, \operatorname{clip}(r, 1-\epsilon, 1+\epsilon)A\right)]
\end{equation}

\chapter{Problem Formulation}
We begin by simply stating the problem that we are studying. The central problem is the optimisation of an objective function over the space of manifold triangulations. Formally given the space of all triangulations of some manifold $\mathcal{M}$, denoted $\mathcal{T}(\mathcal{M})$, and some \textit{objective function} $\mathcal{O}:\mathcal{T}(\mathcal{M})\to\R$ (that is some function we are interested in maximising) we want to find some $T^*\in\mathcal{T}(\mathcal{M})$ subject to some specific size constraint that maximises this function:
\begin{equation}
    T^*=\arg\max_{T\in\mathcal{T}(\mathcal{M}),\operatorname{size}(T)\leq S_{\max}}\mathcal{O}(T)
\end{equation}
Because the space $\mathcal{T}(\mathcal{M})$ is intractably large and poorly understood, finding the global maxima is likely impossible, as are any analytic techniques. Hence, we develop and compare a series of techniques that can be used to find a specific triangulation $T\in\mathcal{T}(\mathcal{M})$ with $\mathcal{O}(T)$ significantly higher than a typical triangulation from $\mathcal{T}(\mathcal{M})$.

To achieve this we address two key sub-problems
\begin{enumerate}
    \item \textbf{Establishing a Baseline:} We need to explore techniques of quantifying for a specific triangulation $T\in\mathcal{T}(\mathcal{M})$ what it means to be significantly better than a typical triangulation in $\mathcal{T}(\mathcal{M})$.
    \item \textbf{Generative Modelling:} We lay the groundwork for a range of new optimisation techniques from the field of reinforcement learning. This involves first establishing a technique that can be used to generate triangulations from $\mathcal{T}(\mathcal{M})$ using deep learning techniques.
\end{enumerate}

The motivation for this work stems from a range of fields where optimising some objective functions over $\mathcal{T}(\mathcal{M})$ is of particular interest. While the techniques we develop are specifically designed to be agnostic to the objective function, we focus primarily on one specific class of objective functions that relate to the knots present in triangulations as a proof of concept.

\chapter{Search and Sampling Algorithms}
\section{Classical Techniques}
\subsection{Direct Accent}\label{sec:direct-accent}
For an objective function $\mathcal{O}:\mathcal{T}(\mathcal{M})\to\R$, for small triangulations (number of tetrahedra around 6 or lower), we can typically calculate $\mathcal{O}$ exhaustively from a census. In doing this, we may find that $\mathcal{O}$ tends to increase with more tetrahedra, this motivates the notion of direct accent.
The concept is to start with some some small triangulation, enumerate all of its neighbours that have more tetrahedra, and then sample one of these randomly, biased towards choosing one with a higher objective. This process can then be repeated to generate a chain of triangulations of increasing size, and the whole process repeated to generate multiple chains.

In order for this search technique to work, we need to be able to do two things.
\begin{enumerate}
    \item Enumerate all the neighbours of a specific triangulation $T'$ that are larger, call this set $N_+(T')$
    \item Convert from $\{\mathcal{O}(T)\;:\;T\in N_+(T')\}$ to some selection probability
\end{enumerate}
To address this first point, we consider $(2-3)$ moves. In order for a $(2-3)$ move to happen we need to have a face that is shared by two distinct tetrahedra, this face will be converted to an edge that is ``perpendicular'' in some sense. Hence, we simply iterate through all $2T$ faces and check if a $(2-3)$ move is possible, performing it if it is. The details are explained in more detail in Altmann and Spreer \cite{Altmann2025}.

\begin{Remark}
    The choice of $(2-3)$ moves is to maintain the number of vertices, $(1-4)$ moves can also be considered.
\end{Remark}

For the second point, we need to convert from a list of scores $\{\mathcal{O}_1, ..., \mathcal{O}_n\}$ to a list of probabilities $\{p_1, ..., p_n\}$ ensuring that the probability is positive and normalised to sum to one. This is a standard problem in machine learning and is addressed by the softmax function
\begin{align}
    p_i &= \frac{e^{\beta O_i}}{\sum_j e^{\beta O_j}}
\end{align}
Where $\beta>0$ is some fixed parameter, often related to the inverse of the ``temperature'' $\beta=1/T$ (where $T$ is the temperature, we will use $\beta$ to avoid confusion with a triangulation), lending from an analogy to thermal physics.

With these defined, we can now detail direct ascent in algorithm~\ref{alg:direct-accent}. The algorithm details a single run of direct ascent, this can be done multiple times to get a range of different ascent paths. So long as $\beta<\infty$ there is a possibility for unique paths. When $\beta=0$ it is essentially a random search, choosing any neighbour independent of the objective functions value. When $\beta=\infty$, the neighbour chosen is the one with the greatest objective of all neighbours.

\begin{algorithm}
\caption{Direct Accent}\label{alg:direct-accent}
\begin{algorithmic}
\State Let $T_0\in\mathcal{T}(\mathcal{M})$ be the initial triangulation.
\State Let $\beta > 0$ be a fixed parameter.
\State Let $N\in \N$ be a fixed chain size.
\State Let $\mathcal{O}:\mathcal{T}(\mathcal{M})\to \R$ be some objective function.
\For{$n=1$ to $N$}
    \State Generate all neighbours $T_{ni}$ of $T_{n-1}$ by applying $(2-3)$ moves.
    \For{each neighbours $T_{ni}$ in the set}
        \State Calculate the selection probability $p_i = \frac{e^{\beta O_i}}{\sum_j e^{\beta O_j}}$.
    \EndFor
    \State Sample $T_n$ from the set of neighbours $\{T_{ni}\}$ with probability $p_i$.
    \EndFor
    \State Return $T_N$
\end{algorithmic}
\end{algorithm}

\subsection{Markov Chain Monte Carlo}
For an objective function $\mathcal{O}:\mathcal{T}(\mathcal{M})\to \R$, we will be interested in knowing what a ``typical'' objective function value looks like. One way to do this is to adapt Markov chain Monte Carlo (MCMC) to the Pachner graph, and generate a uniform sampling of the space, calculating $\mathcal{O}$ on each member of the sample and analysing its statistical properties. The main issue with this is that the Pachner graph is infinite, so its impossible to define a uniform distribution on the space. Furthermore, the number of triangulations for a given number of tetrahedra increases incredibly quickly, and at an unknown rate. Hence, a uniform sampling across all levels would be currently impossible to prove (as the number of triangulations is unknown) and would spend ``more time'' at large triangulations simply because there are more of them. Hence, we modify the problem to the goal of returning a sample that is uniform for a given level, but is otherwise different for different levels, and in particular the probability of sampling a fixed triangulation of size $n$ tends to zero as $n$ gets larger \textbf{faster} than the number of triangulations grows, this ensures that the probability of sampling large triangulations tends to zero, moderating the size of the triangulations sampled. Altmann and Spreeer developed a specific algorithm to do this \cite{Altmann2025}. The algorithm implemented is outlined in \ref{alg:mcmc}. We note that $n(T_s)$ is the number of tetrahedra of $T_s$ and an $i-$neighbour is a triangulation resulting from an $i-$Pachner move for $i=2, 3$ is a $(3-2)$ and $(2-3)$ move respectively. The sampling procedure outlined is proven to be uniform in each level as required. However, as each level has a different probability of being sampled from, a specific level $n$ will be chosen and all other triangulations discarded - giving a uniform sample of the space of triangulations of size $n$.

This algorithm represents a single chain of MCMC, typically multiple chains are run to ensure convergence as in accordance with definition~\ref{def:gelman-rubin}.

\begin{algorithm}
\caption{MCMC for Triangulations}\label{alg:mcmc}
\begin{algorithmic}
\State Let $T_1\in\mathcal{T}(\mathcal{M})$ be the initial triangulation.
\State Let $\gamma=1/k, k\in\mathbb{N}$
\State Let $S$ be the number of samples
\State Let $\mathcal{O}:\mathcal{T}(\mathcal{M})\to \R$ be some objective function.
\For{$s=1$ to $S$}
    \State Sample $u\in U([0, 1])$ uniform
    \If{$u<e^{-\gamma n(T_s)}$}
        \State Set $i:=1$, $m:=2n$
    \Else
        \State Set $i:=2$, $m:=2n-2$
    \EndIf
    \State Enumerate $i-$neighbours $T_1',...,T_\ell'$ of $T_s$
    \State Sample $v\in U([0, 1])$ uniform
    \If{$v<\ell / m$}
        \State Set $T_{s+1}$ to a random choice of $T_1',...,T_\ell'$
    \Else
        \State Set $T_{s+1}$ to $T_{s}$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsection{Simulated Annealing}\label{sec:simulated-annealing}
Because the above Markov Chain Monte Carlo algorithm leads to a uniform distribution, we can use it as a proposal distribution for simulated annealing. This allows us to directly modify the MCMC algorithm to become a simulated annealing algorithm. The concept is to propose a triangulation, compare its objective function to the current objective function, if it is better move to this new triangulation, if it is worse move to the new triangulation with some probability, staying at the same triangulation otherwise. Similar to direct ascent, we need to define some way of calculating the probability of moving in the scenario that it is worse, and similar to direct ascent we chose a function
\begin{align}
    p(o_{p}, o_{c})=e^{-\beta(o_c-o_p)}
\end{align}
Where once again $\beta=1/T$ controls how much randomness is involved. Unlike direct ascent, the value of $\beta$ is usually chosen to vary from iteration to itinerate. This is called a temperature schedule. So we have $\beta_s$ for each step $s$.

The implemented standard simulated aneling algorithm is described in algorithm~\ref{alg:simulated-annealing}.

The key to this algorithm performing successfully is that we have a good choice for $\beta_s$. If it's too high we will get stuck in local maxima, and if its too low we essentially have a random walk which won't achieve much more than MCMC. As such, we pick a ``target acceptance rate''. The ``acceptance rate'' is the percentage of the time that we accept a proposal as opposed to staying still. Higher acceptance rates correspond to random walks, and lower ones correspond to the algorithm sitting in the one place. The ``target acceptance rate'' is what we want the acceptance rate to be. During sampling, we track a exponential moving average estimate for what the current acceptance rate is according to
\begin{align}
    r_s=(1-\alpha)r_{s-1}+\alpha a_s
\end{align}
Where $a_s=0$ if the move was not accepted and $a_s=1$ if the move was accepted. We then compare the moving average acceptance rate $r_s$ to the target acceptance rate $r$ and compute the difference $r_s-r$ and update $\beta$ according to
\begin{align}
    \beta_s=\beta_{s-1}e^{\lambda(r_s-r)}
\end{align}
Here $\lambda$ represents how aggressively to track the target, and $\alpha$ represents how quickly to forget past acceptance rates. This gives us an adaptive simulated annealing.

\begin{algorithm}
\caption{Simulated Annealing}\label{alg:simulated-annealing}
\begin{algorithmic}
    \State Let $T_0$ be some starting triangulation.
    \State Let $S$ be the number of samples.
    \State Let $M$ be some hash table memory.
    \State Let $\mathcal{O}:\mathcal{T}(\mathcal{M})\to \R$ be some objective function.
    \State Let $\beta_s$ be some temperature schedule.
    \For{$s=1$ to $S$}
        \State Propose $T_s'$ from $T_{s-1}$ by using $1-$step of the described MCMC algorithm.
        \State Retrieve $o_{s-1}=\mathcal{O}(T_{s-1})$ from $M$
        \State Check if $o_s=\mathcal{O}(T_s)$ is in $M$, if it is - retrieve it, if not - compute it and store it in $M$.
        \State Compute $\alpha=e^{-\beta_n(o_{s-1}-o_s)}$
        \State Generate some $p\sim U([0, 1])$
        \If{$p\leq\alpha$}
            \State Accept $T_s=T_s'$
        \Else
            \State Let $T_s=T_{s-1}$
        \EndIf
    \EndFor
\end{algorithmic}
\end{algorithm}


\section{Transformers}
\subsection{Base Transformer}
We are interested in being able to sample triangulations independently (unlike MCMC where there is a strong correlation). To achieve this, we implement a mostly standard GPT-2 style autoregressive transformer trained to generate isomorphism signatures. The most notable difference is that we embed each letter of the isomorphism signature as a token. In addition to each of the letters in the isomorhpism signature, we also include a three special tokens \verb|[BOS]|, \verb|[EOS]|, and \verb|[PAD]| for the beginning of the isomorphism signature, end of isomorphism signature, and padding to ensure that all the strings in the batch are the same size for parallelism respectively. The architecture used is described in algorithm~\ref{alg:transformer}. We refer to the list of all possible letters in an isomorphism signature as the vocabulary.

The transformer is trained for autoregressive generation. That is, if the input was \verb|[BOS]|$abc$\verb|[EOS]| we would feed \verb|[BOS]|$abc$ into the transformer and set $abc$\verb|[EOS]| as the target. The transformer predicts a vector of $V$ logits for each letter where $V$ is the size of the vocabulary. Each logit represents the transformed probability of that token coming next. The actual probability for logits $\{l_i\;|\;0\leq i\leq V\}$ can be recovered via
\begin{equation}
    p_i=\frac{e^{l_i}}{\sum_j e^{l_j}}
\end{equation}
The loss is chosen to be the categorical cross entropy which simplifies to $-\log(p_i)$ for $i$ the index of the correct token. This loss is zero if the probability of the correct token being selected is $100\%$ and is $\infty$ if the probability of picking the correct token is $0\%$.

The model is trained towards minimising this loss function with AdamW - described in Algorithm~\ref{alg:adamw}. Once the parameters of the model are fit, an isomorphism signature can be generated by starting with the \verb|[BOS]|, passing it through the transformer and getting the predicted logit vector for the last token, randomly selecting a token according to this vector and appending it to the end of the input, repeating until the \verb|[EOS]| token is generated or some maximum size is reached.

\begin{algorithm}
\caption{IsoSig Transformer}\label{alg:transformer}
\begin{algorithmic}
    \State Let $V$ be the vocabulary size
    \State Let $d_e\in\N$ the embedding dimension
    \State Let $\ell$ be the number of layers
    \State Let $h$ be the number of heads
    \State Let $r\in[0, 1]$ be the dropout rate.
    \State Let $w$ be some partially complete isomorphism signature of size $l\geq 1$
    \State Compute \verb|tok_emb|$\in \R^{l\times d_e}$ as the token embedding of each letter in $w$
    \State Compute \verb|pos_emb|$\in \R^{l\times d_e}$
    \State Compute $X=$\verb|tok_emb|$+$\verb|pos_emb|$\in \R^{l\times d_e}$
    \State Compute $X=DO(X, r)\in\R^{l\times d_e}$
    \For{$i\in[1,...,\ell]$}
        \State $X=TB(X, h, r)\in\R^{l\times d_e}$
    \EndFor
    \State Compute $X=LN(X)\in\R^{l\times d_e}$
    \State Return \verb|logit|$=MX\in \R^{l\times V}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{AdamW}\label{alg:adamw}
\begin{algorithmic}
    \State Let $L:\theta\to\R$ be a loss function to minimise
    \State Let $\beta_1, \beta_2, \eta, \lambda$ be fixed parameters
    \State Let $T$ be the number of steps
    \State Initialise $\theta_0$ randomly
    \For{$t=1$ to $T$}
        \State Compute $g_t=\nabla_\theta L(\theta_{t-1})$
        \State Compute $m_t=\beta_1m_{t-1}+(1-\beta_1)g_t$ and $v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2$
        \State Compute $\hat{m}_t=m_t/(1-\beta_1^t)$ and $\hat{v}_t=v_t/(1-\beta_2^t)$
        \State Update $\theta_t=\theta_{t-1}-\eta\left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t}+\varepsilon}+\lambda \theta_{t-1}\right)$
    \EndFor
\end{algorithmic}
\end{algorithm}

\chapter{Numerical Experiments}
\section{Objective Functions}\label{sec:objective-functions}
To test the effectiveness of our optimisation strategies on triangulations of manifolds, we consider 5 specific objective functions. These objective functions are all considered on single vertex triangulations of $S^3$, call the set of all of these $\mathcal{T}_1(S^3)$.

\subsection{Alexander Polynomial}
For any $T\in\mathcal{T}_1(S^3)$, we can consider an edge $e_\alpha$, because the triangulation has only one vertex, $e_\alpha$ forms a loop. This loop can be knotted in $S^3$. Call the associated knot for $e_\alpha$ $K_\alpha$. The Alexander polynomial $\Delta_{K_\alpha}(t)$ has coefficient vector $\vec{a}=[a_0,a_1,...,a_n]$. It is known that ``more complex'' Alexander polynomials lead to ``more complex'' knots. This notion suggests 3 objective functions of interest.

\begin{enumerate}
    \item $\mathcal{O}_{deg}:T\mapsto\sum_{e_\alpha} \deg(\Delta_{K_\alpha}(t))$ where because the Alexander polynomial is invariant under multiplication by $\pm t$ we use the difference between the highest and lowest power of $t$.
    \item $\mathcal{O}_{det}:T\mapsto\sum_{e_\alpha} |\Delta_{K_\alpha}(-1)|$.
    \item $\mathcal{O}_{norm}:T\mapsto\sum_{e_\alpha} \|\vec{a}\|^2$ where $\vec{a}$ is the coefficient vector of the Alexander polynomial associated with the knot formed by the edge $e_\alpha$ in $S^3$
\end{enumerate}
The first of these objective functions, $\mathcal{O}_{deg}$ is motivated by the fact that $2g(K)\geq \deg(\Delta_K(t))$. The second objective function, $\mathcal{O}_{det}$ is motivated by the connection between the determinant $|\Delta_K(-1)|$ and the knot colouring. The third objective function, $\mathcal{O}_{norm}$ is a standard objective function on vectors.

\subsection{Fundamental Group of $T\backslash\nu( e_\alpha)$}
As discussed, for $T\in\mathcal{T}_1(S^3)$, any edge $e_\alpha$ can form a knot. Typically in knot theory we are interested in the study of the knot complement $M_\alpha=S^3\backslash \nu(K_\alpha)$. This inspires our fourth objective function
\begin{enumerate}[resume]
    \item $\mathcal{O}_{gen}:T\mapsto\sum_{e_\alpha} \#_{gen}(\pi_1(M_\alpha))$ where $\#_{gen}$ is the number of generators of the presentation of the fundamental group.
\end{enumerate}
While we would ideally like to use the reduced presentation of the fundamental group, as discussed in \ref{thm:trivial-presentation-undecidable} there is no algorithm to do this always in finite time, so we use the standard presentation returned by regina \cite{Regina}, as a heuristic, and then a reduction process can be attempted afterwards to check if the fundamental group can be reduced, and if so what it can be reduced to.

\subsection{Edge Degree Variance}
The prior four objective functions are all inspired by the knot structure of our triangulations. While this is of particular interest because it is a topological property of the triangulation, we would also like to consider the complexity of the triangulation itself. This inspires us to consider the degree of the edges in our manifold, where the degree of an edge is the number of tetrahedra that share this edge.
\begin{enumerate}[resume]
    \item $\mathcal{O}_{var}:T\mapsto \operatorname{Var}_{e_\alpha}(\deg(e_\alpha))$ where we have taken the variance.
\end{enumerate}
The choice of variance here follows from the simple argument that $V=1$ because we have defined our space of triangulations to be single vertex triangulations. We know that $\chi=0$ for $S^3$, so $1-E+F-T=0$. Also we know that each of the tetrahedra have $4$ faces, but because $S^3$ is closed they have to be glued together in pairs, so $F=2T$, this means $E=1+T$. Also, each tetrahedra has to contain $6$ edges, so the total edge degree must be $6T$, and the average edge degree $\frac{6T}{T+1}$, hence this is constant for a fixed number of tetrahedra, regardless of the triangulation. Hence the variance is chosen because it captures how ``uniform'' or conversely ``irregular'' the triangulation is.

\section{Markov Chain Monte Carlo}\label{sec:mcmc}
To establish a baseline for our objective functions, generic Markov Chain Monte Carlo was run to determine the distribution of our objective function across the Pachner Graph. The MCMC algorithm described by Altmann and Spreer \cite{Altmann2025} samples uniformly around triangulations of a particular number of tetrahedra. We, choose to examine the Pachner graph around triangulations of 30-tetrahedra, which corresponds approximately to $\gamma=1/10$. This choice is informed by other results in computational topology that have found interesting triangulations of size less than $30$ tetrahedra (for example Burton \cite{Burton2023}), and because $30$ tetrahedra is intractable to enumerate exhaustively. Samples with $7$ chains of $10,000$ iterations with a step size of $100$ where performed, for each sample all objective functions were calculated (as opposed to running distinct sampling processes for each objective function). This has the benefit of not only saving computational time, but giving us a uniform sample of the space where we can asses the correlation of each objective function. We confirmed convergence with the Gelman-Rubin statistic with a threshold of $1.01$, this threshold is in accordance with modern recommendations \cite{Vehtari2021}. The convergence statistics are in table~\ref{tab:mcmc-convergence}. For interest, figure~\ref{fig:obj-correlation} shows the correlation between each objective functions across the entire set of MCMC samples. It is important to note that this correlation tells us the correlation for ``an average'' sample, and the correlation between the functions could dramatically change in very specific parts of the distribution - in particular near the respective maxima of each variable.

We extracted the triangulations with $30$ tetrahedra, there were $8,250$ such triangulations so an efficiency of $\approx 12\%$. The following analyses are performed on this subset of triangulations.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
        Metric & $\hat{R}$ \\
        \hline
        $\mathcal{O}_{norm}$ & 1.003 \\
        $\mathcal{O}_{deg}$ & 1.004 \\
        $\mathcal{O}_{det}$ & 1.005 \\
        $\mathcal{O}_{var}$ & 1.006 \\
        $\mathcal{O}_{gen}$ & 1.009 \\
        \hline
    \end{tabular}
    \caption{Convergence statistics of MCMC Runs}
    \label{tab:mcmc-convergence}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{thesis/figures/obj_correlation.pdf}
    \caption{Correlation plot for each objective function.}
    \label{fig:obj-correlation}
\end{figure}

The distribution for the score for each objective function outlined in section~\ref{sec:objective-functions} is displayed in figure~\ref{fig:mcmc-score-dist}.

All but $\mathcal{O}_{var}$ present power law distributions or power law distributed tails. To confirm this we fit both a power law distribution across the entire data, and also compute $x_{\min}$ according to the procedure of Clauset, Shalizi, and Newman, and fit a power law distribution to the tails. Both these fits are done with MLE. A plot on a log-log histogram of the data, and the fitted distributions is seen in figure~\ref{fig:mcmc-score-dist-log}.

The $\mathcal{O}_{var}$ appears to have some positively skewed distribution, considering that it is the distribution of the variance, a chi squared distribution would be expected. However, fitting one to the data lead to poor results and a log normal distribution was used instead. This is weakly justified, asside from a strong in sample fit as seen in figure~\ref{fig:mcmc-var-score-dist-log} that is plotted in log-log space to confirm tail behaviour.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_alex_deg_hist.pdf}
        \caption{$\mathcal{O}_{deg}$}
        \label{fig:mcmc-score-dist-deg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_alex_det_hist.pdf}
        \caption{$\mathcal{O}_{det}$}
        \label{fig:mcmc-score-dist-det}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_alex_norm_hist.pdf}
        \caption{$\mathcal{O}_{norm}$}
        \label{fig:mcmc-score-dist-norm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_num_gen_hist.pdf}
        \caption{$\mathcal{O}_{gen}$}
        \label{fig:mcmc-score-dist-gen}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_edge_var_hist.pdf}
        \caption{$\mathcal{O}_{var}$}
        \label{fig:mcmc-score-dist-var}
    \end{subfigure}

    \caption{Distribution of the score for each objective function under MCMC samples at $\gamma=1/10$}
    \label{fig:mcmc-score-dist}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_alex_deg_log_distribution.pdf}
        \caption{$\mathcal{O}_{deg}$}
        \label{fig:mcmc-score-dist-log-deg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_alex_det_log_distribution.pdf}
        \caption{$\mathcal{O}_{det}$}
        \label{fig:mcmc-score-dist-log-det}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_alex_norm_log_distribution.pdf}
        \caption{$\mathcal{O}_{norm}$}
        \label{fig:mcmc-score-dist-log-norm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/agg_score_num_gen_log_distribution.pdf}
        \caption{$\mathcal{O}_{gen}$}
        \label{fig:mcmc-score-dist-log-gen}
    \end{subfigure}

    \caption{Log-log histogram of the score for each objective function under MCMC samples at $\gamma=1/10$. The red line indicates the fitted power law distribution over the entire dataset, the orange line the fitted power law distribution for $x>x_{\min}$}
    \label{fig:mcmc-score-dist-log}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{thesis/figures/agg_score_edge_var_log_distribution.pdf}
    \caption{Log-log histogram of $\mathcal{O}_{var}$ with fitted log normal distribution.}
    \label{fig:mcmc-var-score-dist-log}
\end{figure}

\section{Classical Optimisation}
\subsection{Direct Ascent}
We perform direct accent as described in section~\ref{sec:direct-accent} starting from the seed triangulation of \verb`cMcabbgqs` which has one vertex and two tetrahedra. We performed this direct accent 20 times at different temperatures ($\beta = 1/T = 0.1, \sqrt{0.1}$, $1$, $\sqrt{10}$, $10$) and compared the maximum achieved score at each level, an example of this for $\mathcal{O}_{deg}$ in figure~\ref{fig:direct-ascent-temp}. In all cases it was found that a lower temperature lead to better results, this suggests that the graph is connected enough that greedy ascent is sufficient (that is, the neighbour with the greatest score is accepted at each step).

We perform greedy ascent for each objective function for $28$ steps to a triangulation of $30$ tetrahedra, the chains are presented in figure~\ref{fig:direct-ascent-all}.

We see that for each objective function the ascent profile is almost fully deterministic. All objective functions demonstrate monotonic growth. $\mathcal{O}_{deg}$ demonstrates accelerated growth, and $\mathcal{O}_{det}$, $\mathcal{O}_{var}$, and $\mathcal{O}_{norm}$ demonstrate linear growth; in either case this suggests that an arbitrarily high score can be achieved by simply extending this procedure to larger triangulations. On the other hand, $\mathcal{O}_{gen}$ appears to be plateauing suggesting that the exploration technique may not yield significant improvement as we extend to large triangulations. Interestingly, $\mathcal{O}_{var}$ presenting slightly better than expected values for even triangulations than odd triangulations (if we assume linearity), this leads to a slight oscillating behaviour.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{thesis/figures/direct_ascent_temp.pdf}
    \caption{Direct Ascent on $\mathcal{O}_{deg}$ at different temperatures ($\beta=1/T$)}
    \label{fig:direct-ascent-temp}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/direct_ascent_all_deg.pdf}
        \caption{$\mathcal{O}_{deg}$}
        \label{fig:direct-ascent-all-deg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/direct_ascent_all_det.pdf}
        \caption{$\mathcal{O}_{det}$}
        \label{fig:direct-ascent-all-det}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/direct_ascent_all_norm.pdf}
        \caption{$\mathcal{O}_{norm}$}
        \label{fig:direct-ascent-all-norm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/direct_ascent_all_gen.pdf}
        \caption{$\mathcal{O}_{gen}$}
        \label{fig:direct-ascent-all-gen}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/direct_ascent_all_var.pdf}
        \caption{$\mathcal{O}_{var}$}
        \label{fig:direct-ascent-all-var}
    \end{subfigure}

    \caption{Direct / greedy ascent for each objective function.}
    \label{fig:direct-ascent-all}
\end{figure}

While the direct ascent approach does appear to perform particularly well and predictably, and is an incredibly simple algorithm to implement. It only generates a single triangulation, and requires calculating the objective function on a large number of discarded triangulations - in total to generate the ascent profile of 28 triangulations required exploring on the order of $1,000$ triangulations. As such, in situations where a single high value triangulation is desired, this technique demonstrates value. However, if the goal is to explore a variety of different high valued triangulations and have a high exploration efficiency - this technique is not suitable.

\subsection{Simulated Annealing}
We perform simulated annealing as described in section~\ref{sec:simulated-annealing}. This is done for a target of $10,000$ iterations with a step size of $10$ and target acceptance rate of $20\%$, $\alpha=0.01$ and $\lambda=0.1$, this acceptance rate was chosen heuristically based on a number of different small experiments. To ensure a fair comparison with direct ascent, the potential was set to $-\infty$ for triangulations with more than 30 tetrahedra. For $\mathcal{O}_{deg}$ and $\mathcal{O}_{norm}$, the chain was stopped early at $3,307$ and $2,940$ samples because the computational time required to score a single triangulation had become unfeasibly slow ($>10$second$/$sample). The actual acceptance rate was calculated in each case. For $\mathcal{O}_{det}, \mathcal{O}_{gen}$ and $\mathcal{O}_{var}$ the achieved acceptance rate was within $1\%$ of the target. For $\mathcal{O}_{deg}$ and $\mathcal{O}_{norm}$ the acceptance rate is lower, though this is likely due to the reduced number of samples achieved. The chains for each objective function and the achieved acceptance rates are included in figure~\ref{fig:simulated-annealing}.

For $\mathcal{O}_{deg}$ and $\mathcal{O}_{norm}$ which were both stopped early, the objective function appears to still be increasing, suggesting that a higher objective function could have been achieved if let run for the full 10,000 iterations. However, it is important to realise that the more complex a knot is, the longer it takes to calculate the Alexander polynomial, so if the algorithm was to continue to run and improve the objective function, it is likely that it would continue to slow down. The computation for the Alexander polynomial is slow because the symbolic determinant of a large collection of matrices is required to be calculated. This is a poorly optimised problem. However, the full symbolic result is possibly not required, and specific algorithms to calculate the degree and norm could potentially be developed to significantly improve the compute time, and leverage the parallelism of GPUs. $\mathcal{O}_{gen}$, and $\mathcal{O}_{var}$ both appeared to plateau relatively early and did not show significant improvement after around $2,000$ epochs. Similar could be suggested for $\mathcal{O}_{det}$, though the results are less conclusive.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/sim_annealing_deg.pdf}
        \caption{$\mathcal{O}_{deg}$ acceptance rate: $17.78\%$}
        \label{fig:simulated-annealing-deg}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/sim_annealing_det.pdf}
        \caption{$\mathcal{O}_{det}$ acceptance rate: $19.96\%$}
        \label{fig:simulated-annealing-det}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/sim_annealing_norm.pdf}
        \caption{$\mathcal{O}_{norm}$ acceptance rate: $18.44\%$}
        \label{fig:simulated-annealing-norm}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/sim_annealing_gen.pdf}
        \caption{$\mathcal{O}_{gen}$ acceptance rate: $20.11\%$}
        \label{fig:simulated-annealing-gen}
    \end{subfigure}

    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{thesis/figures/sim_annealing_var.pdf}
        \caption{$\mathcal{O}_{var}$ acceptance rate: $19.44\%$}
        \label{fig:simulated-annealing-var}
    \end{subfigure}

    \caption{Trace plots for simulated annealing on each objective function for 10,000 iterations and a $20\%$ acceptance rate.}
    \label{fig:simulated-annealing}
\end{figure}

\section{Comparison of Classical Optimisation}
The best result from each optimisation technique are compiled in table~\ref{tab:classical-results}. This table includes the achieved percentile of the best sample, computed by the fitted power law distribution for both the entire distribution (overall) and the tail distribution (tail) from section~\ref{sec:mcmc}.

In all cases the search techniques uncovered samples that where at least in the $10^{-5}$th percentile. Meaning that $100,000$ samples of $30$ tetrahedra would be required. Given the $30$ tetrahedra sampling rate was on the order of $10\%$ at least $1,000,000$ samples from MCMC would be required to find a solution as good, simulated annealing only used $10,000$, and direct ascent only around $1,000$ making the classical search techniques at least $100\times$ more efficient. In the best case scenario, the score was in the $10^{-13}$th percentile (conservatively choosing the largest of total and tail) making the search at least $10^{10}\times$ more efficient.

With the exception of $\mathcal{O}_{det}$, the simulated annealing performed better than the greedy ascent. That being said, simulated annealing also had to perform $2\times$ to $5\times$ more calculations than the direct ascent (of the $10,000$ steps, the model would sometimes propose an already visited node which didn't need to be recomputed). The improvement was 1-2 orders of magnitude in percentile (conservatively) suggesting that simulated annealing is the better choice when maximising the objective function as much as possible is desired. Though the increased complexity in the algorithm, and the slower $2\times$ to $5\times$ slower runtime are drawbacks if absolute maximums are not required. Interestingly, $\mathcal{O}_{det}$ performed significantly better under direct ascent. Achieving a particularly high percentile on the order $10^{-13}$. It was not entirely clear that simulated annealing for $\mathcal{O}_{det}$ had settled down, and a longer chain may have achieved better results, but it is also possible that the objective function for $\mathcal{O}_{det}$ is convex enough that direct ascent is highly efficient.

For the knot based objective functions, we consider the best triangulations achieved overall from both techniques, identifying which edge within the triangle has the most complex knot (as defined by the objective function).

For $\mathcal{O}_{deg}$ the edge that had the greatest degree had a degree of $1,626$. Additionally, it had a fundamental group that was not of the form $\langle a, b\;|\; a^pb^{-q}\rangle$ and had an Alexander polynomial that could not be decomposed over $\mathbb{Q}$, as such it is not a torus knot, and is likely a prime knot. It could be a satellite knot.

For $\mathcal{O}_{det}$ the edge that had the greatest determinant had a determinant of $57$. The Alexander polynomial factors to $(a^4 - a^3 + a^2 - a + 1) (a^{20} - a^{15} + a^{10} - a^5 + 1)$ and the fundamental group was $\langle a b \;|\; a^{12} b a^{-13} b \rangle$. This could be a $(5,2)-$cable of the $(5,2)-$torus knot. In total for this triangulation, $3$ of the edges where unknots.

For $\mathcal{O}_{norm}$ the edge that had the greatest norm had a norm of $359$. The Alexander polynomial had degree $724$ and fundamental group with three generators. The Alexander polynomial has alternating $+1, -1$ coefficients suggesting a torus knot (the fundamental group may not be simplified)
In total for this triangulation, $3$ of the edges where unknots.

For $\mathcal{O}_{gen}$ the edge that had the greatest number of generators had a $51$ generators, but after simplification only had $3$. The Alexander polynomial had degree $162$. The Alexander polynomial has alternating $+1, -1$ coefficients suggesting a torus knot (the fundamental group may not be fully simplified)
In total for this triangulation, $4$ of the edges where unknots.

For $\mathcal{O}_{var}$ the best triangulation had edges primarily with degree $1, 2$ or $3$ and with one edge of degree $131$. For this triangulation, all edges where unknotted.

\begin{landscape}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        Objective Function & \multicolumn{3}{|c|}{Direct Ascent} & \multicolumn{3}{|c|}{Simulated Annealing} \\
        \hline
         & Score & Total Percentile & Tail Percentile & Score & Total Percentile & Tail Percentile \\
        \hline
        $\mathcal{O}_{deg}$ & $152.26$ & $7.19\cdot 10^{-05}$ & $1.21\cdot 10^{-05}$ & $433.42$ & $2.12\cdot 10^{-05}$ & $2.66\cdot 10^{-06}$ \\
        $\mathcal{O}_{det}$ & $27.19$ & $1.38\cdot 10^{-13}$ & $2.62\cdot 10^{-13}$ & $13.84$ & $5.86\cdot 10^{-11}$ & $1.61\cdot 10^{-10}$ \\
        $\mathcal{O}_{norm}$ & $29.06$ & $7.16\cdot 10^{-12}$ & $3.29\cdot 10^{-06}$ & $152.26$ & $2.38\cdot 10^{-17}$ & $1.23\cdot 10^{-08}$ \\
        $\mathcal{O}_{gen}$ & $1.94$ & $1.58\cdot 10^{-06}$ & $2.32\cdot 10^{-05}$ & $2.65$ & $2.93\cdot 10^{-09}$ & $1.14\cdot 10^{-07}$ \\
        $\mathcal{O}_{var}$ & $412.54$ & $3.06\cdot10^{-15}$ &  & $522.93$ & $7.52\cdot 10^{-18}$ & \\
        \hline
    \end{tabular}
    \caption{Comparison of maximum achieved objective function for both classical sampling techniques. Percentiles of each score based on bulk and tail fitted power law distributions.}
    \label{tab:classical-results}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         & Best Triangulation (\verb|IsoSig|) \\
        \hline
        $\mathcal{O}_{deg}$ & \verb|EMzMzMzMzMvAAMzMzMzMPabcdefghijklmnqpqrstuvwxyzABCDDjxxxxxxxxxxxxxlfxfxxxxxxxxxxxxj| \\
        $\mathcal{O}_{det}$ & \verb|ELLMMzMzMzMzMzMzMzMzQbeddfghijklmnopqrstuvwxyzABCDDDxxrfaxxxxxxxxxxxxxxxxxxxxxxxxvc| \\
        $\mathcal{O}_{norm}$ & \verb|EMzMzMzMzMvAAMzMzMzMPabcdefghijklmnqpqrstuvwxyzABCDDjxxxxxxxxxxxxxlfxfxxxxxxxxxxxxj| \\
        $\mathcal{O}_{gen}$ & \verb|ELwvMvQPLMzAPLLAPMzPQcbehgklimllpnqptrutwvzxyABDCBDDaacmvlpvlooashoqkmbuoageahamkcg| \\
        $\mathcal{O}_{var}$ & \verb|EMzMzMzMzMzMvAAMzMzMPabcdefghijklmnopqsttuvwxyzABCDDvaaaaaaaaaaaaaaaaciqnaaaaaaaaav| \\
        \hline
    \end{tabular}
    \caption{Best Triangulation Direct Ascent}
    \label{tab:t-best-dirct}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         & Best Triangulation (\verb|IsoSig|) \\
        \hline
        $\mathcal{O}_{deg}$ & \verb|ELvLLzvMvzMQAMLMQzQMQbdgfjpmtosmvruqptvwuzyxBCzACBDDafgfjxaovwrdsifjouitrwblqjfhroj| \\
        $\mathcal{O}_{det}$ & \verb|ELLvMLAAzPwMzAPzvAMQQadeihhhljnplnqpturttxBBCADBAADDbabagjgxbadjbhohagkfauahoxrccjo| \\
        $\mathcal{O}_{norm}$ & \verb|EvLPzvvzLAvzLPPQQMQQQdcdgklqsvoAptytAzBxxzCwyBBDzADDonnapdeaacqowdbauqadasckkjabvhd| \\
        $\mathcal{O}_{gen}$ & \verb|ELMvLAAvMAQzLwPLAMMMPcbdgfiihnmloopnqtsvtxwyzyABACDDaaaavlsvaccfbxsaqranmglanxxnxxr| \\
        $\mathcal{O}_{var}$ & \verb|EMLLLvzPPzwPLMQzAPzAQabddglnjokkqnouqswstuvwzyzBCDCDgagbaaagacvaggaccavgfbcafcoaaco| \\
        \hline
    \end{tabular}
    \caption{Best Triangulation Simulated Annealing}
    \label{tab:t-best-sa}
\end{table}
\end{landscape}

\section{Baseline Transformer Efficiency on Isomorphism Signatures}
A dataset of triangulations of $S^3$ is collected for a range of different sizes ($6-$tetrahedra to $12-$tetrahedra). These either come from the census of triangulations \cite{Regina}, or from MCMC. A separate transformer model is trained for each number of tetrahedra. Each transformer has the exact same parameters: an embedding dimension of $64$, $6$ layers, $8$ heads, a dropout rate of $0.1$, and a learning rate of $0.0005$. Each model was trained from the same starting key for $50,000$ iterations, where an iteration is a single batch of size $32$. Each batch was a randomised sample from the training data. A fixed, random, test set of $1,000$ was used for validation. These parameters were chosen as a computationally reasonable starting point for training, though there is significant research to suggest that larger models will perform better, especially given the essentially unlimited training data that we have \cite{Kaplan2020}.

Figure~\ref{fig:sgd-test-losses} shows a log-log plot of the test loss over the number of iterations. It is evident that the curves are still decreasing, and at $50,000$ iterations there isn't significant enough curvature to infer when the training would plateau, this suggests that further training time would yield better results. Regardless of the small architecture and low training time, the data does exhibit promising results.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{thesis/figures/sgd_test_losses.pdf}
    \caption{Caption}
    \label{fig:sgd-test-losses}
\end{figure}

 Figure~\ref{fig:generation-efficiency} and table~\ref{tab:generation-efficiency} show the generation efficiency for the transformers trained on different numbers of tetrahedra. ``Loads'' indicates if the whether the generated string is formatted correctly to be loaded into regina. ``Valid'' checks if the string represents a topologically valid triangulation, i.e. doesn't have any singularities etc. in it. ``Closed'' checks if the manifold is closed (compact and has no boundary). ``Sphere'' checks if the manifold represents a sphere, i.e. if its fundamental group is trivial. In general, of the manifolds that load, almost all of them are valid. Also, of the manifolds that are closed about $70\%$ are spheres - though there is not enough data to suggest if this holds true in general. However, the fraction of valid manifolds that are closed does show evidence that it is decreasing as the number of tetrahedra increases.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \# Tetrahedra & Loads & Valid & Closed & Sphere \\
        \hline
        $6$ & $0.195$ & $0.195$ & $0.115$ & $0.086$ \\
        $7$ & $0.08$ & $0.08$ & $0.039$ & $0.0235$ \\
        $8$ & $0.043$ & $0.0425$ & $0.02$ & $0.0115$ \\
        $9$ & $0.0185$ & $0.0185$ & $0.004$ & $0.004$ \\
        $10$ & $0.008$ & $0.008$ & $0.0025$ & $0.001$ \\
        $11$ & $0.0075$ & $0.0075$ & $<0.001$ & $<0.001$ \\
        $12$ & $0.001$ & $<0.001$ & $<0.001$ & $<0.001$ \\
        \hline
    \end{tabular}
    \caption{Generation efficiency at different number of tetrahedra.}
    \label{tab:generation-efficiency}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{thesis/figures/generation_efficiency.pdf}
    \caption{Caption}
    \label{fig:generation-efficiency}
\end{figure}


\chapter{Discussion and Conclusion}
% \section{Discussion}
% \subsection{Guided Search}



% \section{Future Work}
% \subsection{Reinforcement Learning}
% We have successfully trained transformers to be able to generate triangulations, though the efficiency isn't particularly high. Before deciding to do reinforcement learning on the objective function, it may be beneficial to improve the rate at which the model is generating valid spheres. Apart from increasing the amount of training time, one possible avenue would be to do reinforcement learning with the objective being whether or not the triangulation is a sphere (or to start simply if its valid). In order to apply reinforcement learning, it is generally required that the number of positive samples per batch is $\geq1$. Considering our batch size is $32$, this would mean the efficiency needs to be above $0.03$. This is achieved for $6-$tetrahedra directly for spheres, and for $6, 7, 8-$tetrahedra for the initial task of being valid. The batch size could certainly be increased though this would require more compute power. As such, reinforcement learning is a valid avenue immediately for triangulations of size $6, 7$ and $8$, and would likely apply to larger triangulations after training with gradient descent for longer and with a larger model.

% \subsection{Alternate Isomorphism Signatures}
% It is a well empirically observed result that increasing the embedding dimension increases performance for transformers. This can be explained with two factors. Firstly, in general more parameters of any form gives the model more freedom and nuance allowing it to fit to the data better. Secondly, and more pertinent, the embedding layer is the component of the model that learns to express what each letter ``means''. With the current isomorphism signatures, they are simply representations of numbers, so specific letters don't contain any geometric meaningful information, the model has to ``waste effort'' learning how to decode the isomorphism signatures, which is a relatively sophisticated encoding structure. A different encoding strategy could be used to represent the triangulations where each token has a specific and unique meaning. One possibility is ``dehydrated isomorphism signatures''. These have a potential advantage because each pair of letters encodes a unique piece of geometric information, and the position encodes the gluing information. This is far more subtile for the specific task that transformers were developed to handle and using such a signature could lead to faster training times, and higher accuracy for the same sized model. Additionally, there could be potential interest in the embedding vectors learned themselves.

% \subsection{Alternative Architectures}

\section{Discussion}
% Paragraph 1: Summarise the primary success of the classical methods.
This study examines single vertex triangulations of the three sphere. We successfully implement and compare classical heuristic search algorithms against a baseline established with MCMC. The primary result of our analysis is that classical heuristics are able to find exceptional triangulations, often with knot theoretic properties falling with the $10^{-10}$th percentile or rarer compared to the baseline distribution. In general we find that simulated annealing approaches work better than greedy ascent, suggesting that in order to navigate the Pachner graph efficiently, a degree of randomness is needed, indicating that the landscape has non-convex structure with local optima / locally optimal paths.

% Paragraph 2: Integrate your point about the limitations of the objective functions.
While the optimisation techniques are powerful, their success is intrinsically linked to the objective function chosen. During the development of the optimisation techniques, there was interest in understanding if guided search could be used to discover specific, special types of triangulations. In particular, we were interested to see if we could use guided search to discover a triangulation where all the edges were knotted. Burton had used targeted search to find an example where this was true. The concept was that if we find a triangulation which exhibits particularly high ``knottedness'' in general, it may be more likely to have all of its edges knotted. However, after running optimisation on a range of different objective functions, we found that we could achieve much higher values for the objective functions than what was actually obtained by the triangulation discovered by Burton. That is, the objective functions did not discriminate enough to be used to locate such a triangulation. This is not a failure in the optimisation techniques, but rather in the definition of the objective function. In general, if the discussed optimisation techniques are to be used for searching for specific triangulations, it needs to be done on some objective function that is known to be optimised by a hypothetical solution. For example - the number of edges unknotted is known to be minimised by a triangulation of the desired form (tautologically), unfortunately it is too flat to actually serve any value for searching (tending to ``hover'' around 2).

The main current main limitations of the presented search techniques are twofold. Firstly, they are local search techniques. They generate chains of samples that are all quite similar leading to extended periods of sampling triangulations that all have similar objective functions, thereby wasting compute not gaining any significant new information. We do lay the groundwork for deep learning based approaches that may combat this issue, but further work needs to be done to determine if they are viable. Secondly, the objective functions used become increasingly computationally intensive as the solutions we are examining become more optimal. This is somewhat unavoidable, but there is possible wasted compute determining the full polynomial Alexander polynomial, for it to later be reduced to a single number. This could potentially be saved and open the door for the computation to become ``embarrassingly parallel'', lending itself to modern GPU hardware.

% Paragraph 3: Discuss the transformer experiments as a proof-of-concept and its current limitations.
To address the sequential limitation of the classical search techniques, we investigated generative machine learning techniques, namely autoregressive transformers trained on the isomorphism signatures of the triangulations. We have demonstrated initial proof of concepts with these models on small triangulations, but find diminishing success with larger triangulations. This diminishing success is expected, and there is evidence that the accuracy can simply be improved with longer training times. The need for increased training times is a bottle neck for developing models to generate large / complex triangulations. In particular, to open up the avenue for reinforcement learning we need to push the initial accuracy above some minimum threshold, which is currently only being exhibited on small triangulations.

\section{Conclusion}
This report has provided a range of approaches to navigating the Pachner graph. We have successfully demonstrated that classical search heuristics dominate uniform random sampling for uncovering particularly exceptional triangulations with complex knot structures. The methods provide a practical and objective function agnostic approach to exploring the Pachner graph that may be implemented easily. Further, we have laid the groundwork to open the door to deep learning, successfully demonstrating a proof of concept generative transformer that can successfully generate triangulations. While there are still challenges with the efficiency of the generative techniques, there is evidence that the efficiency can be improved to the level required for reinforcement learning - a technique that we believe has the opportunity for great success.

\section{Future Work}
The results and limitations discussed above suggest a range of possible avenues to move forward, primarily around improving the generative model and applying reinforcement learning. Particularly to see if it outperforms simulated annealing.

\subsection{Advancing the Generative Model}
The immediate priority should be to improve the generative efficiency of the model. There are two avenues forward for this.

Firstly, reinforcement learning can be used to improve the generative efficiency. That is, we can use it as a fine tuning techniques to encourage the generation of valid, closed, or spherical triangulations. As the results have demonstrated, for small triangulations of $6-8$ tetrahedra, this technique could already be directly applied as the generative efficiency is already in the $2-3\%$ range required for to get positive reward signals in a batch size of $32$.

Secondly, it is a well empirically observed result that increasing the embedding dimension increases performance for transformers. This can be explained with two factors. Firstly, in general more parameters of any form gives the model more freedom and nuance allowing it to fit to the data better. Secondly, and more pertinent, the embedding layer is the component of the model that learns to express what each letter ``means''. With the current isomorphism signatures, they are simply representations of numbers, so specific letters don't contain any geometric meaningful information, the model has to ``waste effort'' learning how to decode the isomorphism signatures, which is a relatively sophisticated encoding structure. A different encoding strategy could be used to represent the triangulations where each token has a specific and unique meaning. One possibility is ``dehydrated isomorphism signatures''. These have a potential advantage because each pair of letters encodes a unique piece of geometric information, and the position encodes the gluing information. This is far more subtile for the specific task that transformers were developed to handle and using such a signature could lead to faster training times, and higher accuracy for the same sized model. Additionally, there could be potential interest in the embedding vectors learned themselves.

\subsection{Alternative Architectures}
If changing the representation of the isomorphism signature, and increasing computational time is found to be insufficient, there are alternative avenues forwards with the actual neural network architecture.

Firstly, trying to generate the isomorphism signature letter by letter is a highly fragile procedure, at each step there may be a probability of making a mistake and generating a single invalid character, after doing this repeatedly the chance of making a single mistake which would invalidate the entire isomorphism signature compounds. And once a mistake is made there is no ``going back''. This suggests the possibility of training diffusion style models, these can be trained to require less steps to generate a result, and can also be designed to be able to correct errors that have been made, based on the context of the entire string. This error correcting ability is potentially of significant value for improving efficiency.

Secondly, a triangulation is fundamentally a graph type structure, using graph aware or graph neural networks may be a better choice for the data than sequential style models. A graph style model could be more suited to quickly uncovering the underlying structure and rules of a valid triangulation, meaning smaller networks could be trained faster to achieve equivalent efficiencies.

\subsection{Reinforcement Learning for Topological Optimisation}
The main next steps once generation efficiency has been improved, is to perform reinforcement learning to improve the topological objective function outlined above, and compare them to the results of the classical optimisation. The improved generative model would be trained with the objective functions explored in this report as reward signals. Using algorithms such as proximal policy optimisation (PPO), the model could directly be trained to generate novel triangulations that maximise knot theoretic or combinatorial properties. This could be a powerful new approach to the problem that involves models that can actually encode the geometry, as opposed to heuristic search algorithms.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
